Anaconda / pycharm 同属于集成编译环境IDE   但不同的是pycharm需要引进pip位置  
numpy
pandas
scipy 


几乎所有的机器学习算法最后都归结为求解最优化问题，
以达到我们想让算法达到的目标。

按照用途和要解决的问题，
机器学习算法可以分为有监督学习，无监督学习，强化学习3种类型。
有监督学习又进一步细分为  分类问题与回归问题，
无监督学习算法分为  聚类问题和数据降维问题。


hashlib.md5
已经安装解释器和常见的包
np.random.rand 
x = np.array([100,20,34])
x.size  = 3  代表数量 
多种方式来创建数组，random.rand只是其中一种  最终结果都是Numpy数组

pd.DataFrame
中心极限定理  正太分布
掌握基本方法

概率对模型是积极相关的
因果颠倒  贝叶斯公式 
两点分布 0-1分布  伯努利分布
二项分布   负二项分布
泰勒展开
泊松分布  和数数相关
均匀分布
指数分布  无记忆性 前面没发生问题，后面也不会发生问题
正态分布 高斯分布
Beta分布
指数族
期望 方差 协方差  标准差 相关性
Pearson相关系数
切比雪夫不等式  大数定理
中心极限定理
最大似然估计（理论和实践相吻合  可能是对的）----》参数估计



数据清洗 特征选择  【机器学习中比较重要 最终决定模型的好坏】
排出污数据与矫正
Pandas  数据读取和处理
 4.3 重要  涉及pandas 对数据的处理

有监督学习：
身高 、温度 、 连续值  ：线性回归    房屋价格

 可以对样本是非线性的 只要对参数 进行线性即可
离散值  ：分类    鸢尾花  【 逻辑回归  分类问题首选算法】  多分类：Softmax  逻辑回归的推广
logistic回归  Sigmoid函数  【释义：S形状的】
逻辑回归正常是一条线 无法得到曲线  但可以通过维度提升解决
逻辑回归本身是一个线性模型  但是我们本身还可以得到非线性的结论
似然函数？


过拟合 ？

权重值比较大，不容易控制


Ridge回归   LASSO回归


测试数据不能太少 ，但同时也要保证训练数据要多一些

梯度下降算法  参数过高  每次找梯度进行下降  逐步找到最低值

批量  每次拿出一批样本平均梯度进行下降  mini-batch
随机  每次拿一个样本更改梯度下降  速度快 时间短


------------决策树和随机森林-------------
少数服从多数 对叶子节点进行分类
对随机事件的信息量求期望得熵   条件熵【平均熵】降低多少表达了条件对Y 的信息增益

通过信息增益【选择特征】对决策树进行分割 
for i in [1,2,3,4]:
	gain(Y,Xi) = H(Y) - H(Y|Xi)
main(gain(Y,Xi))
信息增益率:抑制分叉的个数
Gini系数 ：信息熵做一阶的近似  【基尼系数】
确定性事,,件的信息熵就是0  p=0（完全不发生）/p=1（完全发生）的信息熵就是0
熵值比较大，证明不确定性比较大
决策树容易过拟合

样本和特征上做随机   

Out  OF  Bag  OOB  袋外数据


不均衡性 
A 990   降采样
B 10     过采样
大类【A】进行降采样 


990  --》0.05 =49   10
990                          10*20  = 200

1.pydotplus 安装
2.graphaviz 安装 包
3.安装 graphaviz 软件  设置path路径


----------支持向量机SVM---------
n个样本  M个特征
如何做？
如何求解？得出参数 

线性可分支持向量机  ---》松质因子》线性支持向量机 
利用支撑向量得到一个机器，则称作支持向量机

w*x+b = 0 

C/γ  :大  正确率提高、泛化能力降低（更容易过拟合）

正确率以及无法分别了就考虑泛化能力
什么是SVM呢？
样本到直线距离最小值  【所有】中的最大的那条线最优结果
拉格朗日乘子法

α=0 非支撑向量
α》0 支撑向量

过渡带越宽 越防止过拟合，提高泛化能力

线性支持向量机

核函数映射   高维空间


----------聚类----------- 无监督的学习算法
以前x特征  Y标记  
Y是连续  ：回归     
      离散 ：分类

Y是缺失的、不存在的  不方便获取的  无监督的学习算法  往往是聚类

只能依靠样本的自相似性分类   每个类别称为簇  
类别内数据相似度较大，类别间数据相似度较小

相似度如何度量？ /距离算法
两点之间距离 ：欧氏距离


K-均值
K-中值
MiNi-Batch K-Means
层次聚类方法   自下而上【合并】      自上而下【分裂】两种方法



-------------EM算法------------- 无监督分类算法
数据缺失 某一列 某几列  
或者由于我们数据建模时导致数据缺失
高斯混合模型GMM  
最大似然估计 推到EM
推到GMM参数   --- 多元高斯  --拉格朗日乘子法 


通过身高数据来进行性别预测，
男 Gauss(u1,sigma1)   n1   p1
女 Gauss(u2,sigma2)   n2   p2

GMM是我们要去建造的模型  算法使用EM算法

Jensen不等式  f凸函数

西方国家把往下凹的函数叫做convex（凸） function，
而苏联教材叫做вогнутая（凹） функция。

y=loga(x)  (0<a<1)

后验概率：是指在得到“结果”的信息后重新修正的概率（或者说给出某个条件后）

K-means没有办法给出任何一个样本属于这个簇的后验概率

最大似然估计：找出与样本分布最接近的概率分布模型

期望表示随机变量的中心位置。
方差用于表示数据的分散程度。数据波动越大，方差就越大。
高斯 严格  欧拉 近似

概率描述了已知参数时的随机变量的输出结果；
似然则用来描述已知随机变量输出结果时，未知参数的可能取值。
极大似然估计，就是在已知观测的数据的前提下，找到使得似然概率最大的参数值。

EM：期望最大值


pLSA模型  概率隐喻义分析模型


性别、身高、体重

有监督 x1  x2  ....xn  yn
无监督  x1  x2  ....xn
把样本认为是离散的或者相互独立的  认为xi和xj之间没有关系
但是有些时候数据不是这样

时间序列：不管时间相关空间相关的序列
股票、温度变化等与有关时间序列相关的

隐马尔可夫模型：进行时间序列预测的 
再语音识别、NLP、生物信息、模式识别等领域有效 
概率计算   动态规划
参数估计 
模型预测
上述三个问题是息息相关的

中文分词算法实践：
1.给定词典 2.HMM


可尔科夫：一阶模型、二阶模型。。。。多阶模型
	今天          昨天
	明天          今天
	                 明天
HMM3个基本问题
概率计算问题   前后-后向算法
	直接算法  ： 暴力算法
	前向算法
	后向算法
学习问题 EM算法

预测问题  动态规划 

训练数据只有观测序列和状态序列 则HMM算法是进啊都学习

训练数据只有观测序列，则HMM需要使用EM算法  是非监督学习

Vertibi算法

做事情要有人管你，是很幸福的事情，自己摸索是很痛苦的

jieba进行切词 


在一定意义下，数据比算法更重要


------主题模型LDA------贝叶斯模型

信息提取和搜索
文档分类/聚类  文章摘要、社区挖掘
基于内容的图像聚类、目标识别
生物信息数据的应用

需要注意：
模型是什么原理？哪些参数需要我们调整？

应用方向：
主题模型  PLSA   两层贝叶斯网络
	LDA    三层

LDA隐Dirichlet模型

给定一定文本，提取文本主题或者标题top model

降维：n1个词  -》  20 个主题
软聚类：经典聚类无法计算后验算法    ||    lda、E没算法可以

伽马函数   P(x) = (x-1)!
Beta分布

共轭先验分布
给定样本计算模型的参数

Dirichlet分布


LDA 一个文档对应多个主题  一个主题对应多个词  
	我们要做的就是对隐变量的挖掘









